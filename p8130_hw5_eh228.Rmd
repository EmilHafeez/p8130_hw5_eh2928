---
title: "P8130 Biostatistical Methods Homework 5"
author: "Emil Hafeez (eh2928)"
date: "11/13/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library("animation")
library(arsenal)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(knitr)
library(broom)
library(faraway)

theme_set(theme_minimal() + theme(legend.position = "bottom")) #setup and establish the colors schemes for plits
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

```{r, results = 'hide'}
#Read the CSV data into a dataframe
antibodies_df <- read.csv("./data/Antibodies.csv")

# Make AgeCategory, Smell, and Gender appopriate datatypes. Helps to ensure we know all unique values, too.
unique(antibodies_df$AgeCategory)
    antibodies_df <- antibodies_df %>% 
        mutate(AgeCategory = factor(AgeCategory, labels = c("18-30", "31-50", "51+") ))
unique(antibodies_df$Smell)
    antibodies_df <- antibodies_df %>% 
        mutate(Smell = factor(Smell, levels = c("Normal","Altered", "Unanswered/Others")))
    antibodies_df = antibodies_df %>% filter(Smell != "Unanswered/Others")
unique(antibodies_df$Gender)
    antibodies_df <- antibodies_df %>% 
        mutate(Gender = factor(Gender, levels = c("Male","Female")))
```

In order to assess the difference in IgM levels between the two smell factor groups (Normal vs Altered) given non-normal distributions, we opt for a non-parametric test called the Wilcoxon Rank-Sum test. It's the nonparametric equivalent of the two-sample independent t-test, and examines if the medians of the two populations are equal versus not equal:

\(H_0 = \) the medians of the two groups' IgM levels are equal, and \(H_A = \) the medians of the two groups' IgM levels are not equal. The decision rule is that we reject \(H_0 = \) if \(T > z_{1-(\alpha/2)} \). 

The test statistic is computed, with a continuity correction, one of two ways. We first combine the data from the two groups, order the values from lowest to highest, assign ranks to the individual values (1 to n), and if ties, assign the average rank. Then, select a group and compute the sum of ranks \(T_1\) for the first group, and then use the appropriate test statistic formula. 

With no ties (referring to two equally ranked values once the values are listed), the test statistic is

\(T=\frac{\left|T_{1}-\frac{n_{1}\left(n_{1}+n_{2}+1\right)}{2}\right|-\frac{1}{2}}{\sqrt{\left(n_{1} n_{2} / 12\right)\left(n_{1}+n_{2}+1\right)}}\)

and with ties, the test statistic is

\(T=\frac{\left|T_{1}-\frac{n_{1}\left(n_{1}+n_{2}+1\right)}{2}\right|-\frac{1}{2}}{\sqrt{\left(n_{1} n_{2} / 12\right)\left[\left(n_{1}+n_{2}+1\right)-\sum_{i=1}^{g} t_{i}\left(t_{i}^{2}-1\right) /\left(n_{1}+n_{2}\right)\left(n_{1}+n_{2}-1\right)\right]}}\)

where \(t_i\) refers to the number of observations with the same absolute value in the \(i^{th}\) group and \(g\) is the number of tied groups.

In our case, the test statistic calculated by R is slightly different, since it does not by default add the  \(n_1(n_1+1)/2\) term (and is denoted by W).  

The p-value under the normal approximation, with \(n_1\) and \(n_2\) \(\geq 10\) is described by \(2 * [1 -\Phi(T)] \). 

```{r wilcoxon rank-sum}
antibodies_df2 =
  antibodies_df %>% 
  pivot_wider(
		names_from = Smell,
		values_from = Antibody_IgM
	)

wilcox.test(antibodies_df2$Normal, antibodies_df2$Altered, mu = 0)
```

In context, and ignoring missing values and the unanswered smell category, we find evidence to reject the null hypothesis and conclude that the true location shift between the Normal and Altered smell categories is not equal to zero (in other words, the median IgM values are different for the two groups). 


# Problem 2

## Problem 2 Part 1

\(L\left(\mu, \sigma^{2} \mid Y\right)=\prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^{2}}} \cdot e^{-\frac{\left(Y_{i}-\mu\right)^{2}}{2 \sigma^{2}}}\)

\(L\left(\beta_{0}, \beta_{1}, \sigma^{2}\right)=\prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^{2}}} \cdot e^{-\frac{\left(Y_{i}-\beta_{0}-\beta_{1} X_{i}\right)^{2}}{2 \sigma^{2}}}=\left(2 \pi \sigma^{2}\right)^{-n / 2} \cdot e^{\frac{-\sum_{i=1}^{n}\left(Y_{i}-\beta_{0}-\beta_{1} X_{i}\right)^{2}}{2 \sigma^{2}}}\)

\(\begin{array}{l}
\ln L\left(\beta_{0}, \beta_{1}, \sigma^{2}\right)=\log \left[\left(2 \pi \sigma^{2}\right)^{-\frac{n}{2}} \cdot e^{\left.\frac{-\sum_{i=1}^{n}\left(Y_{i}-\beta_{0}-\beta_{1} X_{i}\right)^{2}}{2 \sigma^{2}}\right]}\right. \\
\ln L\left(\beta_{0}, \beta_{1}, \sigma^{2}\right)=-\frac{n}{2} \log (2 \pi)-n \log (\sigma)-\sum_{i=1}^{n} \frac{\left(Y_{i}-\beta_{0}-\beta_{1} X_{i}\right)^{2}}{2 \sigma^{2}}
\end{array}\)

\(\ln L\left(\beta_{0}, \beta_{1}, \sigma^{2}\right)=\)

\(\begin{array}{l}
\frac{d}{d\beta_1} ln(\beta_0, \beta_1,\sigma) = -\frac{1}{2 \sigma^{2}} \frac{d}{d \beta_1} \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta{1}\right)^{2} \\
=\frac{1}{\sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta{1}\right)
\end{array}\)

\(0 = \frac{1}{\sigma^{2}} \Sigma_{i=1}^n(Y_i - \beta_1)\)

\(\beta_1 = \frac{1}{n} \Sigma_{i=1}^n(Y_i - \beta_1)\)

\(=\frac{cov{(X Y)}}{var({X})}\)

Segmented

\(\hat\beta_0 = \bar{y} - \hat\beta_1\bar{x}\)


/ / / / / / / / / / / / / / / / / / / / / / / / / / / / / /
Let's try that again. 

\(\begin{array}{l}
\mathrm{SSE}=\sum_{i}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2} \\
=\sum_{i}^{n}\left[y_{i}-\left(\beta_{1} x_{i}+\beta_{0}\right)\right]^{2} \\
=\sum_{i}^{n}\left(y_{i}-\beta_{1} x_{i}-\beta_{0}\right)^{2}
\end{array}\)

\(
\begin{array}{l}
\frac{\partial}{\partial \beta_{0}} \mathrm{SSE}=-2 \sum_{i}^{n}\left(y_{i}-\beta_{1} x_{i}-\beta_{0}\right) \\
=-2 \sum_{i}^{n} y_{i}+2 \beta_{1} \sum_{i}^{n} x_{i}+2 n \beta_{0} \\
=-2 n \bar{y}+2 \beta_{1} n \bar{x}+2 n \beta_{0}
\end{array}\) and then solving for the intercept when we set the derivative equal to 0 = 
\(\beta_0 = \bar{y} - \beta_1\bar{x}\)

"We used the fact that the sum over the values of a variable is equal to the mean of those values multiplied by how many values we have."

Then, we can utilize this result to find the slope \(\beta_1\) by using the intercept solution and isolating the slop term, such that

\(\begin{array}{l}
\frac{\partial}{\partial \beta_{1}} \mathrm{SSE}=-2 \sum_{i}^{n}\left(y_{i}-\beta_{1} x_{i}-\beta_{0}\right) x_{i} \\
=-2 \sum_{i}^{n} x_{i} y_{i}+2 \beta_{1} \sum_{i}^{n} x_{i}^{2}+2 \beta_{0} \sum_{i}^{n} x_{i} \\
=-2 \sum_{i}^{n} x_{i} y_{i}+2 \beta_{1} \sum_{i}^{n} x_{i}^{2}+2\left(\bar{y}-\beta_{1} \bar{x}\right) \sum_{i}^{n} x_{i} \\
=-2 \sum_{i}^{n} x_{i} y_{i}+2 \beta_{1} \sum_{i}^{n} x_{i}^{2}+2 \bar{y} \sum_{i}^{n} x_{i}-2 \beta_{1} \bar{x} \sum_{i}^{n} x_{i} \\
=2 \beta_{1} \sum_{i}^{n} x_{i}^{2}-2 \beta_{1} \bar{x} \sum_{i}^{n} x_{i}+2 \bar{y} \sum_{i}^{n} x_{i}-2 \sum_{i}^{n} x_{i} y_{i} \\
=2 \beta_{1}\left(\sum_{i}^{n} x_{i}^{2}-\bar{x} \sum_{i}^{n} x_{i}\right)+2 \bar{y} \sum_{i}^{n} x_{i}-2 \sum_{i}^{n} x_{i} y_{i}
\end{array}\)


# Problem 3

## Problem 3 Part 1

Load the data and plot it.
```{r}
gpa_df <- read.csv("./data/GPA.csv")

plot(gpa_df$ACT, gpa_df$GPA)
```
Test whether a linear association exists between ACT score (x) and GPA at the end of freshman year (Y). 

Let \(\alpha = 0.05\) and let \(\beta_1\) represent the true slope to be estimated. 

The null hypothesis is \(H_0: \beta_1 = \beta_{10}\) where \(\beta_{10} = 0\). The alternative hypothesis is \(H_A: \beta_1 \neq \beta_{10}\). In context, testing \(H_0:\beta_1 = 0\) examines whether a student's GPA at the end of freshman year can be predicted from the ACT test score.

The test statistic follows the t distribution with n-2 degrees of freedom, such that

\(t=\frac{\widehat{\beta_{1}}-\beta_{10}}{s e\left(\widehat{\beta_{1}}\right)} \sim t_{n-2}, \text { under } H_{0} = \frac{0.03883 - 0}{0.01277} = 3.040\) using degrees of freedom = \(n = 120, df = n-2\) 

The corresponding critical value is fixed by \(t_{n-2, 1-(\alpha/2)}\) and the decision rule is that we reject \(H_0\) if \(|t| > t_{n-2, 1-(\alpha/2)}\) and fail to reject \(H_0\) if \(|t| \leq  t_{n-2, 1-(\alpha/2)}\). As such, the critical value is \(t_{118, 0.975} = 1.980272\).

Therefore, \(|t| > t_{118, 0.975}\) using the 5% significance level, we find evidence to reject the null hypothesis and conclude that there is a significant linear association between students' ACT scores and GPA at the end of freshman year. 

```{r}
reg_admit<-lm(gpa_df$GPA ~ gpa_df$ACT, data = gpa_df)

# Summarize regression
summary(reg_admit)

tidy(reg_admit)

glance(reg_admit)


# Scatterplot and regression line overlaid
plot(gpa_df$ACT, gpa_df$GPA)
abline(reg_admit,lwd=2,col=2)

```

## Problem 3 Part 2

The basic regression model follows the form \(Y_i = \beta_0 + \beta_1X_i + \epsilon_i\) and the estimated regression model is given by \(\widehat{Y} = \widehat{\beta_0} + \widehat{\beta_1}X_i, i = 1,2,3...n\). In our context, this means that the estimated GPA value is equal to the sum of the intercept \(\beta_0\) and the estimated beta one times the student's ACT score, such that \(\widehat{GPA} = 2.11405 + 0.03883 \cdot ACT\).

## Problem 3 Part 3

The 95% confidence interval for the true slope \(\beta_1 \) is given by \(\widehat{\beta_1} \pm t_{n-2, 1-(\alpha/2)} \cdot se(\widehat{\beta_1}) \)where \(se(\widehat{\beta_1}) = \sqrt{MSE / \Sigma_{i=1}^n}(X_i - \overline{X})^2 \). 

In our context, the 95% confidence interval for the true slope is equal to \(0.03883 \pm 1.980272 \cdot 0.01277 = (0.01354, 0.06412)\). This confidence interval does not include 0. The Director of Admissions may be interested in whether this confidence interval includes 0 because 0 is the null value. As it stands, with 95% confidence we estimate that the student GPA score increases by between (0.06412, 0.01354) points for each additional ACT point scored priorly. If the interval were to include 0, we may suspect that the true increase in student GPA score per additional ACT points may include 0, which is to say that the ACT may have no utility informing our admissions process. Perhaps there is other evidence we may consider that informs whether we retain ACT requirements, too, though. 

## Problem 3 Part 4

The 95% confidence interval for the mean freshman GPA for students whose ACT score is 28 is given by: 

$$
\begin{array}{c}
\widehat{\beta_{0}}+\widehat{\beta_{1}} X_{h} \pm t_{n-2,1-\alpha / 2} \cdot \operatorname{se}\left(\widehat{\beta_{0}}+\widehat{\beta_{1}} X_{h}\right) \\
\operatorname{se}\left(\widehat{\beta_{0}}+\widehat{\beta_{1}} X_{h}\right)=\sqrt{M S E\left\{\frac{1}{n}+\left[\left(X_{h}-\bar{X}\right)^{2} / \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}\right]\right\}}
\end{array}
$$

```{r}
reg_admit <- lm(GPA ~ ACT, data = gpa_df)

ACT_to_predict_from = data.frame(ACT = 28)

predict(reg_admit, ACT_to_predict_from, interval = "confidence")

```

In context, this means that with 95% confidence we estimate the mean freshman GPA for students whose ACT scores are 28 to be between 3.061384 and 3.341033 (3.061384, 3.341033). 

## Problem 3 Part 5

The 95% prediction interval for Anne's freshman GPA is calculated as below.

$$
\begin{array}{c}
\widehat{\beta_{0}}+\widehat{\beta_{1}} X_{h} \pm t_{n-2,1-\alpha / 2} \cdot \operatorname{se}\left(\widehat{\beta_{0}}+\widehat{\beta_{1}} X_{h}\right) \\
\operatorname{se}\left(\widehat{\beta_{0}}+\widehat{\beta_{1}} X_{h}\right)=\sqrt{M S E\left\{\frac{1}{n}+\left[\left(X_{h}-\bar{X}\right)^{2} / \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}\right] + 1\right\}}
\end{array}
$$
```{r}
reg_admit <- lm(GPA ~ ACT, data = gpa_df)

ACT_to_predict_from = data.frame(ACT = 28)

predict(reg_admit, ACT_to_predict_from, interval = "prediction")
```

We predict with 95% confidence that Anne's freshman year GPA will be between 1.959355 and 4.443063, as predicted from her ACT test score of 28. 

## Problem 3 Part 6

Prediction interval will always be wider than the CI, because it has an additional term to account for (the non-normally distributed error term). This is visible in the above formulas, where the confidence interval and prediction interval are calculated using similar formats, but the standard errors are quite different: the prediction interval focuses on one specific new value of \(Y_h\), and since we do not calculate an expected mean the errors do not reduce to 0, and so the SE formula for prediction includes a \(+1\) in the denominator, widening the interval overall.
